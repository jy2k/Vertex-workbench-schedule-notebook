{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5786316f-6f84-403a-b62d-cbf49fdeccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To push the branch back to git you will probably need a personal access token\n",
    "#https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token\n",
    "\n",
    "#Before pushing to repo don't forget to right click the file on the left pane and git add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afffe79e-d764-402c-ab98-92c7b9e19abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3f786-234b-492f-8c64-74aa8e88b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download data\n",
    "clientBQ = client(credentials=)\n",
    "dataframe =clientBQ.query() #/ snowflake / SQL\n",
    "\n",
    "#use this to train later on\n",
    "#https://cloud.google.com/vertex-ai/docs/training/understanding-training-service\n",
    "#Instead of using tfds.load you can download the dataset from anywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bf0c3a6-e4e1-4c5b-a4f4-b391e8f6dfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "data, info = tfds.load(name='cifar10', as_supervised=True, with_info=True)\n",
    "NUM_CLASSES = info.features['label'].num_classes\n",
    "DATASET_SIZE = info.splits['train'].num_examples\n",
    "print(DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b55e542-3b83-4f2a-9c39-f14625ae3f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(image, label):\n",
    "  image = tf.image.resize(image, (300,300))\n",
    "  return tf.cast(image, tf.float32) / 255., label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04d44b35-c9c1-4af3-9963-2437ec5c6353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/validation splits\n",
    "\n",
    "# Shuffle dataset\n",
    "dataset = data['train'].shuffle(1000)\n",
    "\n",
    "train_split = 0.8\n",
    "val_split = 0.2\n",
    "train_size = int(train_split * DATASET_SIZE)\n",
    "val_size = int(val_split * DATASET_SIZE)\n",
    "\n",
    "train_data = dataset.take(train_size)\n",
    "train_data  = train_data.map(preprocess_data)\n",
    "train_data  = train_data.batch(64)\n",
    "\n",
    "validation_data = dataset.skip(train_size)\n",
    "validation_data  = validation_data.map(preprocess_data)\n",
    "validation_data  = validation_data.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e0a1561-421f-4c6c-b39f-dff35f89320f",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "feature_extractor_model = \"inception_v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3d67a3a-e976-441e-afef-e566e8fb768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_hub_uri = f\"https://tfhub.dev/google/imagenet/{feature_extractor_model}/feature_vector/5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a36fd04b-be39-45a2-9438-fa6ab92c9de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_layer = hub.KerasLayer(\n",
    "    tf_hub_uri,\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1dc6d6a-8bd5-4856-8ad3-8fedac38f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  feature_extractor_layer,\n",
    "  tf.keras.layers.Dense(units=NUM_CLASSES)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2be4d35-4cce-47c5-a4ac-8e3b484ce40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 08:24:21.613006: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/13 [======================>.......] - ETA: 21s - loss: 2.1005 - acc: 0.2922"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 08:25:35.270372: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - ETA: 0s - loss: 2.0105 - acc: 0.3375"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(),\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['acc'])\n",
    "\n",
    "model.fit(train_data, validation_data=validation_data, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d967c497-f78f-4d4c-9913-74d370f811f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this should create a local folder with all the assets\n",
    "#https://www.tensorflow.org/guide/keras/save_and_serialize\n",
    "model.save(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e217a-568e-4d0f-b7e8-ff83187fbffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://cloud.google.com/storage/docs/uploading-objects\n",
    "from google.cloud import storage\n",
    "\n",
    "bucket_name = \"automl-output-mlops\"\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # The path to your file to upload\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # The ID of your GCS object\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        f\"File {source_file_name} uploaded to {destination_blob_name}.\"\n",
    "    )\n",
    "    \n",
    "#TODO: fill in the arguments to use the funciton above\n",
    "source_file = \"my_model/saved_model.pb\"\n",
    "dest_blob = \"saved_model.pb\"\n",
    "upload_blob(bucket_name, source_file, dest_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff7917-5de1-4227-8b78-2cbdd8935639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://cloud.google.com/vertex-ai/docs/model-registry/import-model#pre-built-container\n",
    "#https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/upload_model_sample.py\n",
    "\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "PROJECT_ID = \"mlops-demos-306914\"\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID)\n",
    "            \n",
    "def upload_model_to_model_registry(\n",
    "    project: str,\n",
    "    display_name: str,\n",
    "    serving_container_image_uri: str =\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\", # https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "    location: str = \"us-central1\",\n",
    "    artifact_uri: Optional[str] = None,\n",
    "    serving_container_predict_route: Optional[str] = None,\n",
    "    serving_container_health_route: Optional[str] = None,\n",
    "    description: Optional[str] = None,\n",
    "    serving_container_command: Optional[Sequence[str]] = None,\n",
    "    serving_container_args: Optional[Sequence[str]] = None,\n",
    "    serving_container_environment_variables: Optional[Dict[str, str]] = None,\n",
    "    serving_container_ports: Optional[Sequence[int]] = None,\n",
    "    instance_schema_uri: Optional[str] = None,\n",
    "    parameters_schema_uri: Optional[str] = None,\n",
    "    prediction_schema_uri: Optional[str] = None,\n",
    "    explanation_metadata: Optional[explain.ExplanationMetadata] = None,\n",
    "    explanation_parameters: Optional[explain.ExplanationParameters] = None,\n",
    "    sync: bool = True,\n",
    "):\n",
    "\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=display_name,\n",
    "        artifact_uri=artifact_uri,\n",
    "        serving_container_image_uri=serving_container_image_uri,\n",
    "        serving_container_predict_route=serving_container_predict_route,\n",
    "        serving_container_health_route=serving_container_health_route,\n",
    "        instance_schema_uri=instance_schema_uri,\n",
    "        parameters_schema_uri=parameters_schema_uri,\n",
    "        prediction_schema_uri=prediction_schema_uri,\n",
    "        description=description,\n",
    "        serving_container_command=serving_container_command,\n",
    "        serving_container_args=serving_container_args,\n",
    "        serving_container_environment_variables=serving_container_environment_variables,\n",
    "        serving_container_ports=serving_container_ports,\n",
    "        explanation_metadata=explanation_metadata,\n",
    "        explanation_parameters=explanation_parameters,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    model.wait()\n",
    "\n",
    "    print(model.display_name)\n",
    "    print(model.resource_name)\n",
    "    return model\n",
    "\n",
    "#TODO: fill in the arguments to use the funciton above\n",
    "upload_model_to_model_registry(project=PROJECT_ID,display_name=\"my_display_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9984f-5d75-4776-b99d-8cc4bcd53339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/googleapis/python-aiplatform/blob/2bc9b2b0d048c29ba43c8b4c3ea51370515d08c3/samples/model-builder/create_batch_prediction_job_sample.py\n",
    "from typing import Sequence, Union\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "def create_batch_prediction_job_sample(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_resource_name: str,\n",
    "    job_display_name: str,\n",
    "    gcs_source: Union[str, Sequence[str]],\n",
    "    gcs_destination: str,\n",
    "    sync: bool = True,\n",
    "):\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    my_model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "    batch_prediction_job = my_model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        gcs_source=gcs_source,\n",
    "        gcs_destination_prefix=gcs_destination,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    batch_prediction_job.wait()\n",
    "\n",
    "    print(batch_prediction_job.display_name)\n",
    "    print(batch_prediction_job.resource_name)\n",
    "    print(batch_prediction_job.state)\n",
    "    return batch_prediction_job\n",
    "\n",
    "create_batch_prediction_job_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83312cd9-5fe2-416e-b6fc-4c7cd498bd94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a9a5a-4c60-4b59-8c0c-a7623f1ae81b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e1885c-133b-4cba-aed3-d11371e9914e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu:latest"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2 (Local)",
   "language": "python",
   "name": "local-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
